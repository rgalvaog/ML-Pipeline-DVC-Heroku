'''
train_model.py

Rafael Guerra
Feb 2022

This script splits the data into training and testing sets and trains the model generated by model.py on this data.

'''

# Import packages
from model import train_model,compute_model_metrics,inference,sliced_inference
from data import process_data
from sklearn.model_selection import train_test_split
import os
import pickle
import pandas as pd

# Add code to load in the data.
data = pd.read_csv(os.getcwd() + "/data/clean_census.csv",index_col=False)

# Split data into train and test
train, test = train_test_split(data, test_size=0.20,random_state=123456)

cat_features = [
    "workclass",
    "education",
    "marital-status",
    "occupation",
    "relationship",
    "race",
    "sex",
    "native-country"
]

# Process the training data
X_train, y_train, encoder, lb = process_data(
    train, categorical_features=cat_features, label="salary", training=True
    )


# Proces the test data with the process_data function.
X_test,y_test,encoder,lb = process_data(
    test,categorical_features=cat_features,label='salary',training=False,encoder=encoder,lb=lb
)

# Train and save a model.
trained_model = train_model(X_train,y_train)

# Predict with model
predictions = inference(trained_model,X_test)
print(predictions)

# Save model pickle files (model, enconder, labeler)
with open(os.getcwd() + "/model/model.pkl", "wb") as f:
    pickle.dump(trained_model, f)
with open(os.getcwd() + "/model/encoder.pkl", "wb") as f:
    pickle.dump(encoder, f)
with open(os.getcwd() + "/model/lb.pkl", "wb") as f:
    pickle.dump(lb, f)

# Generate slice_output.txt
sliced_inference(cat_features,test.reset_index(drop=True),y_test,predictions)
